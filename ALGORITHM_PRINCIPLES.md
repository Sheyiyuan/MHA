# Metropolis-Hastings (MH) 算法数学原理说明

## 目录
1. [算法背景与目标](#1-算法背景与目标)
2. [核心数学原理](#2-核心数学原理)
3. [MH算法的数学公式](#3-mh算法的数学公式)
4. [马尔可夫链理论基础](#4-马尔可夫链理论基础)
5. [提议分布的选择](#5-提议分布的选择)
6. [本仓库的具体实现](#6-本仓库的具体实现)
7. [参数调优的数学指导](#7-参数调优的数学指导)
8. [收敛性与诊断](#8-收敛性与诊断)

---

## 1. 算法背景与目标

### 1.1 问题描述

在贝叶斯统计、统计物理和机器学习中，我们经常需要从一个复杂的概率分布 π(x) 中进行采样，但是：
- 该分布的归一化常数难以计算或不可计算
- 直接从该分布采样困难（如多峰分布、高维分布）

**例子**：贝叶斯推断中的后验分布
```
π(θ|D) = p(D|θ)p(θ) / p(D)
```
其中 p(D) = ∫ p(D|θ)p(θ)dθ 通常是高维积分，无法直接计算。

### 1.2 MH算法的目标

Metropolis-Hastings (MH) 算法是一种**马尔可夫链蒙特卡洛 (MCMC)** 方法，其目标是：
- 构造一个马尔可夫链 {X₀, X₁, X₂, ...}
- 使得该马尔可夫链的平稳分布恰好是目标分布 π(x)
- 通过大量采样逼近 π(x) 的特征（均值、方差、分位数等）

**关键优势**：只需要知道 π(x) 的**未归一化形式** π̃(x)，即：
```
π(x) = π̃(x) / Z，其中 Z = ∫ π̃(x)dx
```
MH算法不需要知道归一化常数 Z。

---

## 2. 核心数学原理

### 2.1 细致平衡条件 (Detailed Balance)

MH算法的核心是构造一个满足**细致平衡条件**的马尔可夫转移核 K(x'|x)：

```
π(x) K(x'|x) = π(x') K(x|x')    对所有 x, x'
```

**数学意义**：
- 如果从状态 x 转移到 x' 的概率流等于从 x' 转移到 x 的概率流
- 那么 π(x) 就是该马尔可夫链的平稳分布

**证明**：对细致平衡条件两边对 x 积分：
```
∫ π(x) K(x'|x) dx = ∫ π(x') K(x|x') dx
π(x') ∫ K(x|x') dx = π(x')
```
这表明 π(x') 是不变分布（满足 π = πK）。

### 2.2 MH转移核的构造

MH算法通过两步构造转移核：

**步骤1：提议步骤 (Proposal Step)**
- 从提议分布 Q(x'|x) 生成候选状态 x'

**步骤2：接受-拒绝步骤 (Acceptance-Rejection Step)**
- 以概率 α(x, x') 接受候选状态
- 如果拒绝，则保持当前状态

MH转移核的完整形式：
```
K(x'|x) = Q(x'|x) α(x, x') + δ(x' - x) r(x)
```
其中：
- α(x, x') 是接受概率
- δ(x' - x) 是Dirac delta函数（表示保持不变）
- r(x) = 1 - ∫ Q(x'|x) α(x, x') dx' 是拒绝概率

---

## 3. MH算法的数学公式

### 3.1 接受概率公式

为了满足细致平衡条件，MH算法定义接受概率为：

```
α(x, x') = min(1, π(x')Q(x|x') / (π(x)Q(x'|x)))
```

**等价形式**（对数空间，数值更稳定）：
```
log α(x, x') = min(0, log π(x') - log π(x) + log Q(x|x') - log Q(x'|x))

α(x, x') = min(1, exp(log α(x, x')))
```

### 3.2 公式推导

要使细致平衡条件成立：
```
π(x) Q(x'|x) α(x, x') = π(x') Q(x|x') α(x', x)
```

假设 π(x)Q(x'|x) < π(x')Q(x|x')，则令：
- α(x, x') = π(x')Q(x|x') / (π(x)Q(x'|x))
- α(x', x) = 1

代入验证：
```
左边 = π(x) Q(x'|x) · π(x')Q(x|x') / (π(x)Q(x'|x))
    = π(x') Q(x|x')

右边 = π(x') Q(x|x') · 1
    = π(x') Q(x|x')

左边 = 右边 ✓
```

### 3.3 对称提议分布的简化

如果提议分布是**对称的**，即 Q(x'|x) = Q(x|x')，则：
```
α(x, x') = min(1, π(x') / π(x))
```

这就是**Metropolis算法**（MH的特例），公式更简洁。

---

## 4. 马尔可夫链理论基础

### 4.1 马尔可夫性质

马尔可夫链满足**无记忆性**：
```
P(Xₙ₊₁ = x' | X₀, X₁, ..., Xₙ) = P(Xₙ₊₁ = x' | Xₙ)
```
当前状态只依赖于前一个状态，与更早的历史无关。

### 4.2 遍历性 (Ergodicity)

一个马尔可夫链是**遍历的**，如果它满足：
1. **不可约性** (Irreducibility)：从任意状态可以到达任意其他状态
2. **非周期性** (Aperiodicity)：状态的访问不具有周期性
3. **正常返** (Positive Recurrence)：期望返回时间有限

**遍历定理**：如果马尔可夫链是遍历的，且平稳分布为 π(x)，则：
```
lim (n→∞) (1/n) Σᵢ₌₁ⁿ f(Xᵢ) = E_π[f(X)] = ∫ f(x)π(x)dx
```
即：时间平均 = 空间平均（期望）

这就是为什么可以用样本均值估计目标分布的期望。

### 4.3 燃烧期 (Burn-in Period)

由于马尔可夫链从初始状态 X₀ 开始，前期样本可能不服从平稳分布 π(x)。因此需要：
- 丢弃前 B 个样本（燃烧期）
- 只使用 {X_{B+1}, X_{B+2}, ...} 进行统计推断

**数学依据**：马尔可夫链的收敛速度
```
||P^n(x, ·) - π(·)||_{TV} ≤ C ρⁿ
```
其中 ρ < 1 是谱间隙，n 足够大时误差指数衰减。

---

## 5. 提议分布的选择

### 5.1 对称提议分布

#### 5.1.1 高斯提议 (Gaussian Proposal)

**定义**：
```
Q(x'|x) = N(x'|x, σ²I)
```
即 x' = x + ε，其中 ε ~ N(0, σ²I)

**对称性证明**：
```
Q(x'|x) = (1/√(2πσ²)) exp(-(x'-x)²/(2σ²))
Q(x|x') = (1/√(2πσ²)) exp(-(x-x')²/(2σ²))
Q(x'|x) = Q(x|x') ✓
```

**接受概率**：
```
α(x, x') = min(1, π(x') / π(x))
```

**标准差 σ 的选择**：
- σ 太小：接受率高，但探索慢（局部移动）
- σ 太大：接受率低，效率低（频繁拒绝）
- **经验法则**：调整 σ 使接受率在 20%-50% 之间

#### 5.1.2 均匀提议 (Uniform Proposal)

**定义**：
```
Q(x'|x) = Uniform(x - δ, x + δ)
```
即在 [x-δ, x+δ] 区间内均匀采样

**对称性**：
```
Q(x'|x) = 1/(2δ)  if |x' - x| ≤ δ，否则为 0
Q(x|x') = 1/(2δ)  if |x - x'| ≤ δ，否则为 0
Q(x'|x) = Q(x|x') ✓
```

**接受概率**：
```
α(x, x') = min(1, π(x') / π(x))
```

**步长 δ 的选择**：
- 对于离散状态（如骰子）：δ = 1（相邻状态转移）
- 对于连续状态：根据目标分布尺度调整

### 5.2 非对称提议分布

对于非对称提议（如独立提议 Q(x'|x) = q(x')），需要完整的MH公式：
```
α(x, x') = min(1, π(x')q(x) / (π(x)q(x')))
```

---

## 6. 本仓库的具体实现

### 6.1 基类 `BaseMetropolisHastings`

#### 6.1.1 核心算法流程

```python
def _step(self):
    # 1. 生成候选状态（从提议分布）
    x' = proposal_generate(x)
    
    # 2. 计算目标分布的对数概率
    log_π(x) = target_log_prob(x)
    log_π(x') = target_log_prob(x')
    
    # 3. 计算接受概率（对数空间）
    log_α = log_π(x') - log_π(x) + log(Q(x|x')/Q(x'|x))
    α = min(1, exp(log_α))
    
    # 4. 接受-拒绝决策
    if U(0,1) ≤ α:
        x ← x'  # 接受
    else:
        x ← x   # 拒绝（保持不变）
    
    # 5. 记录样本
    samples.append(x)
```

#### 6.1.2 数学公式对应

| 代码方法 | 数学公式 |
|---------|---------|
| `target_log_prob(x)` | log π̃(x) |
| `proposal_generate(x)` | x' ~ Q(x'|x) |
| `proposal_log_prob_ratio(x, x')` | log[Q(x|x')/Q(x'|x)] |
| `_compute_accept_prob(...)` | α = min(1, exp(log_α)) |

### 6.2 高斯提议实现 `GaussianMH`

#### 6.2.1 目标分布（双峰分布）

**数学定义**：
```
π̃(x) ∝ exp(-((x-2)²/2)) + exp(-((x+2)²/8))
```

**对数形式**（数值稳定版本）：
```python
def target_log_prob(x):
    return -((x-2)**2)/2 - ((x+2)**2)/8
```

注意：这里使用的是**对数空间的加权和**，实际分布是两个高斯的混合：
- 第一个峰：均值=2，方差=1
- 第二个峰：均值=-2，方差=4

#### 6.2.2 高斯提议生成

```python
def proposal_generate(x):
    return x + N(0, σ²)
```

**数学原理**：
- 提议分布：Q(x'|x) = N(x'|x, σ²)
- 对称性：Q(x'|x) = Q(x|x')
- 接受概率：α = min(1, π̃(x')/π̃(x))

#### 6.2.3 接受率分析

对于高斯提议，接受率取决于：
1. **步长 σ**：σ 越小，接受率越高
2. **目标分布的曲率**：曲率大（变化快）时接受率低

**理论接受率**（单峰高斯目标）：
- 最优接受率约为 23.4%（Roberts等，1997）
- σ_optimal ≈ 2.38 × std(π) / √d （d为维度）

### 6.3 均匀提议实现 `UniformMH`

#### 6.3.1 目标分布（骰子例子）

**数学定义**（离散分布）：
```
π̃(x) ∝ w(x), x ∈ {1,2,3,4,5,6}
w(1)=1, w(2)=2, w(3)=3, w(4)=2, w(5)=1, w(6)=0
```

**归一化**：
```
Z = Σ w(x) = 1+2+3+2+1+0 = 9
π(x) = w(x) / 9
```

**对数形式**：
```python
def target_log_prob(x):
    weights = {1:1, 2:2, 3:3, 4:2, 5:1, 6:0}
    return log(weights[x] + ε)  # ε防止log(0)
```

#### 6.3.2 均匀提议生成

```python
def proposal_generate(x):
    return x + Uniform(-δ, δ)
```

**数学原理**：
- 提议分布：Q(x'|x) = Uniform(x-δ, x+δ)
- 对称性：Q(x'|x) = Q(x|x') = 1/(2δ)
- 接受概率：α = min(1, π̃(x')/π̃(x))

#### 6.3.3 离散状态处理

对于骰子（整数状态），需要：
1. **取整操作**：x' ← round(x + Uniform(-1, 1))
2. **状态验证**：确保 x' ∈ {1,2,3,4,5,6}
3. **边界处理**：拒绝越界的候选状态

**数学意义**：相当于在离散状态空间上定义随机游走：
```
P(x' = x+1 | x) = P(x' = x-1 | x) ≈ 1/2
```

---

## 7. 参数调优的数学指导

### 7.1 接受率与效率的权衡

**混合时间 (Mixing Time)**：马尔可夫链达到平稳分布的时间
```
τ_mix ∝ 1 / (接受率 × 步长²)
```

**最优策略**：
- 高接受率 + 小步长 → 慢探索
- 低接受率 + 大步长 → 低效率
- **折中**：中等接受率 + 适当步长

### 7.2 有效样本量 (Effective Sample Size, ESS)

由于MCMC样本具有**自相关性**，独立样本量小于总样本量：
```
ESS = N / (1 + 2Σ_{k=1}^∞ ρ(k))
```
其中 ρ(k) 是滞后k的自相关系数。

**目标**：通过调参最大化 ESS/N（有效性）。

### 7.3 Gelman-Rubin诊断

通过多条独立链检验收敛性：
```
R̂ = √(Var_between + Var_within) / Var_within
```
- R̂ ≈ 1：收敛良好
- R̂ > 1.1：需要更长燃烧期或调整参数

---

## 8. 收敛性与诊断

### 8.1 收敛判据

**理论保证**：如果马尔可夫链满足遍历性，则：
```
lim (n→∞) ||P^n(x, ·) - π(·)|| = 0
```

**实践判据**：
1. **轨迹图**：样本路径应充分探索状态空间
2. **自相关图**：自相关应快速衰减至0
3. **多链比较**：不同初始值的链应收敛到相同分布

### 8.2 常见收敛问题

#### 8.2.1 慢混合 (Slow Mixing)

**原因**：
- 步长过小：局部困住
- 多峰分布：峰之间跳转困难

**解决方案**：
- 增大步长
- 使用跳跃提议（如Langevin动力学）
- 平行回火 (Parallel Tempering)

#### 8.2.2 周期行为 (Periodic Behavior)

**原因**：提议分布设计不当（如确定性模式）

**解决方案**：添加随机性

### 8.3 本仓库的收敛保证

#### 8.3.1 高斯提议 (GaussianMH)

- **遍历性**：高斯提议的支撑集是全空间，满足不可约性
- **收敛速度**：几何遍历（指数收敛）
- **建议燃烧期**：1000-2000步（对于1维双峰分布）

#### 8.3.2 均匀提议 (UniformMH)

- **离散状态**：相邻状态转移确保不可约性
- **收敛速度**：依赖于状态空间大小
- **建议燃烧期**：1000步（对于骰子例子）

---

## 9. 数学推广与扩展

### 9.1 多维情况

对于高维参数 x ∈ ℝᵈ：
- **高斯提议**：x' = x + N(0, Σ)，其中 Σ 是协方差矩阵
- **维度诅咒**：随着维度增加，接受率下降
- **解决方案**：Gibbs采样、Hamiltonian Monte Carlo (HMC)

### 9.2 自适应MH (Adaptive MH)

**思想**：在运行中调整提议分布参数
```
σₙ₊₁ = σₙ × exp((αₙ - α_target) / n^γ)
```
其中 α_target 是目标接受率（如0.234），γ ∈ (0.5, 1]。

**理论保证**：需要满足渐减自适应条件（Diminishing Adaptation）。

### 9.3 与其他MCMC方法的关系

| 方法 | 与MH的关系 | 适用场景 |
|------|-----------|---------|
| **Gibbs采样** | MH的特例（接受率=1） | 条件分布易采样 |
| **Hamiltonian MC** | 使用梯度信息的MH | 高维连续空间 |
| **Langevin动力学** | 基于梯度的提议 | 光滑目标分布 |

---

## 10. 参考文献

1. Metropolis, N., et al. (1953). *Equation of State Calculations by Fast Computing Machines*. Journal of Chemical Physics, 21(6), 1087-1092.

2. Hastings, W. K. (1970). *Monte Carlo Sampling Methods Using Markov Chains and Their Applications*. Biometrika, 57(1), 97-109.

3. Roberts, G. O., Gelman, A., & Gilks, W. R. (1997). *Weak Convergence and Optimal Scaling of Random Walk Metropolis Algorithms*. The Annals of Applied Probability, 7(1), 110-120.

4. Gelman, A., et al. (2013). *Bayesian Data Analysis* (3rd ed.). CRC Press.

5. Brooks, S., et al. (2011). *Handbook of Markov Chain Monte Carlo*. CRC Press.

---

## 附录A：本仓库的数学符号表

| 符号 | 含义 | 代码对应 |
|------|------|---------|
| π(x) | 目标分布（归一化） | - |
| π̃(x) | 目标分布（未归一化） | `target_log_prob` 返回 log π̃(x) |
| Q(x'|x) | 提议分布 | `proposal_generate` 采样自 Q |
| α(x, x') | 接受概率 | `_compute_accept_prob` |
| X₀, X₁, ... | 马尔可夫链状态序列 | `samples` 数组 |
| B | 燃烧期长度 | `burn_in` |
| N | 总迭代次数 | `n_iter` |
| σ | 高斯提议标准差 | `proposal_sigma` |
| δ | 均匀提议步长 | `step_size` |

---

## 附录B：数学公式快速查阅

### MH核心公式
```
α(x, x') = min(1, π(x')Q(x|x') / (π(x)Q(x'|x)))
```

### 对称提议简化（Metropolis）
```
α(x, x') = min(1, π(x') / π(x))
```

### 对数空间计算（数值稳定）
```
log α = min(0, log π(x') - log π(x) + log Q(x|x') - log Q(x'|x))
α = exp(log α)
```

### 细致平衡条件
```
π(x) K(x'|x) = π(x') K(x|x')
```

### 遍历定理（大数定律）
```
lim (n→∞) (1/n) Σᵢ₌₁ⁿ f(Xᵢ) = ∫ f(x)π(x)dx
```

---

## 结语

本文档详细阐述了Metropolis-Hastings算法的数学原理，包括：
1. **理论基础**：细致平衡、马尔可夫链遍历性
2. **核心公式**：接受概率的推导与计算
3. **具体实现**：高斯提议与均匀提议的数学细节
4. **实践指导**：参数调优与收敛诊断

希望本文档能帮助读者深入理解本仓库中MH算法的数学本质。
